{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "955a0013",
   "metadata": {},
   "source": [
    "## Naked AI\n",
    "\n",
    "Way back in 1999, a new cooking show in the UK launched called [\"The Naked Chef\"](https://en.wikipedia.org/wiki/The_Naked_Chef) starring [Jamie Oliver](https://en.wikipedia.org/wiki/Jamie_Oliver). The name comes from the style of cooking where the methods were stripped down to the bare essentials.\n",
    "\n",
    "I like the approach of getting down to the basics to begin the learning journey for something as complicated and seemingly magical as AI. I have used this approach successfully before to learn networking and the TCP/IP stack. \n",
    "\n",
    "If we were to strip AI (deep learning specifically) down to the bare essentials, what might we find? If we were to put AI under a microscope, what would we be looking at. If you are looking to better understand things like ChatGPT under the covers, where do you even start? The field of AI began approximately 80 years ago - who has time to study the history and read all the papers to understand how we got where we are today?\n",
    "\n",
    "As I began my journey to learn this technology, I was not getting what I needed just by following [PyTorch](https://pytorch.org/) or [TensorFlow](https://www.tensorflow.org) tutorials. Although I was able to get things working, I remained curious as to what is actually happening underneath the covers. I also felt initially overwhelmed and bored with the concepts, the theories and the dizzying amount of algorithms related to how machines learn. For me at least, I need to learn by doing.\n",
    "\n",
    "The intent here is to provide you with a good yet very basic understanding of an artifical neural network (ANN). Consider it a place to start your learning. We will not cover training - we will assume the network is already trained. In fact, we won't even go into neural networks at all - we will go deeper and see what a stand-alone artificial neuron (aka perceptron) does. This is focused on helping developer's learn AI, and so I will be using code to demonstrate the concepts. Specifically, I'm using Python - if you are not familiar with Python, don't worry - it should be fairly easier to understand if you are familiar with other programming languages. I use Python because it has emerged as the standard programming language for machine learning.\n",
    "\n",
    "If you want to learn more, I recommend the \"MIT Introduction to Deep Learning\" available for free on [YouTube](https://www.youtube.com/watch?v=QDX-1M5Nj7s&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI) - under the [Alexander Amini channel](https://www.youtube.com/@AAmini).\n",
    "\n",
    "In addition, I really like the \"Introduction to Neural Networks\" which is part of [Brilliant's](https://brilliant.org/home/) \"CS & Programming\" course. \n",
    "\n",
    "So, let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1f02d3",
   "metadata": {},
   "source": [
    "### The perceptron\n",
    "Here, we have a perceptron (loosely based on a neuron nerve cell in the human brain). This is what (artificial) neural networks are built from. At the lowest level, it's how ChatGPT works at the cellular level. There are potentially billions of these wired together to form a neural network. This may even look scary at first, but let's see what is going on here. Basically, we have inputs coming in (denoted by X) to a perceptron. \n",
    "\n",
    "Not all inputs carry the same weight - some may be more important than others. It is these weights that are determined (learned) during the training of a neural network. By the way, the term weights is often used interchangeably with the term parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c77186",
   "metadata": {},
   "source": [
    "![Neuron](perceptron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf8717d",
   "metadata": {},
   "source": [
    "In this generic model of a perceptron, we have an unspecified number of inputs denoted by m. Each input (X) has a coresponding weight (W).\n",
    "\n",
    "The perceptron produces a single output denoted by $\\hat{y}$. Unless this is the final output , the output is fed into one or more downstream perceptrons hence the term \"network\".\n",
    "\n",
    "But what does a perceptron do and how does it produce an output? It turns out, all it does it some quite simple math. Here is the equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf97bfe",
   "metadata": {},
   "source": [
    "$\\hat{y} = g ( \\sum \\limits _{i=1} ^{m} x_{i} w_{i}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3f0ef6",
   "metadata": {},
   "source": [
    "This by itself may look a little daunting, so let's break it down further with examples. Let's start with the bit in the brackets:\n",
    "$\\sum \\limits _{i=1} ^{m} x_{i} w_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fa69d9",
   "metadata": {},
   "source": [
    "In plain english, we are multiplying each input (X) by it's weight (W), and adding them all up together (summation).\n",
    "\n",
    "Let's assume we have 3 inputs $X_1, X_2, X_3$ with the coresponding values of 10,14,-5. We have coresponding weights $W_1, W_2, W_3$ with the coresponding values of 1,8,3 (refresher the $\\cdot\\$ is used to represent multiplication):\n",
    "\n",
    "= $(X_1 \\cdot W_1) + (X_2 \\cdot W_2) + (X_3 \\cdot W_3)$\n",
    "\n",
    "= $(10 \\cdot 1) + (14 \\cdot 8) + (-5 \\cdot 3)$\n",
    "\n",
    "= 10 + 112 + (-15)\n",
    "\n",
    "= 107"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bea339c",
   "metadata": {},
   "source": [
    "For the purpose of simplicity, we are ignoring \"bias\" altogether here. Bias is simply an adjustable numerical term added to the sum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab55ab4",
   "metadata": {},
   "source": [
    "Now, onto the next part - what is the weird $g()$? in our formula and why do we need it? Can we not just say the output $\\hat{y}$ be set to 107 and be done? The answer is firmly no. We need to introduce some non-linearity into our network. The reason for this is real-world data is non-linear - roads and rivers have curves and bends - it turns out, so does data. In simplistic terms, if you were to plot the value of houses with 1, 2, 4, 8, 16 bedrooms, would it be a straight line? Most likely not. So, we have to introduce some non-linearity into the network to better reflect the real-world. We may also want to constrain the output to a nice range between 0 and 1 to represent things like probability.\n",
    "\n",
    "There are many standard off-the-shelf non-linear activation functions to choose from. In our simple example below, we use a sigmoid function which is simply a squashing function. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c47c182",
   "metadata": {},
   "source": [
    "### g() - the activation function\n",
    "\n",
    "Recall the value 107 produced earlier by our simple example above. We need to introduce some non-linearity and we decided we will use a [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) function which is simply a squashing function so that the value is mapped to some value between 0 and 1. We are going to leave the math formulae behind and instead jump straight into code. The only library I do use is NumPy and I use it just to do some of the math for me in a succinct way. Here's what it looks like in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "491e3092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cac0c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480b9d53",
   "metadata": {},
   "source": [
    "So, if we were to run 107 through the sigmoid function, we see it returns 1.0 - meaning that it is fully activated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ed2a6e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(107)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329a94ee",
   "metadata": {},
   "source": [
    "And that is it. Our perceptron is complete. See below:\n",
    "![Neuron](perceptron_example.png)\n",
    "\n",
    "In a real (artifical) neural network, the value 1.0 would typically be fed in as an input into one or more other perceptrons along with other outputs from other perceptrons until we reach the final output layer which yields the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a95a0f",
   "metadata": {},
   "source": [
    "### Re-cap\n",
    "\n",
    "Ok, at this point, we've stripped down AI to the basics and put it under a micro-scope. What we found is a thing called a perceptron which is similar to a neuron in the human brain. We've introduced a bit of basic mathematics (Linear Algebra) and we have provided a concrete example and worked through the equation. Things may not be clicking at this point. It's just a bunch of numbers you might have observed. You are absolutely right - it is indeed just a bunch of numbers and some simple math. You might be asking, \"I don't see any signs of intelligence\" and you would also be completely correct in that observation.\n",
    "\n",
    "The illusion of intelligence - the magic of AI, is really how it learns what those weights should be during the training phase. That is out of scope for this little tutorial.\n",
    "\n",
    "Now it's time to solidify our understanding by a) using code and b) an applied example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9580f1ab",
   "metadata": {},
   "source": [
    "Up until this point, we have been a little abstract. Maybe this is interesting, maybe not. Maybe this is still confusing to you. Personally, I always struggled with pure mathematics - light bulbs only go off for me when it is put to work to solve real world problems.\n",
    "\n",
    "This example is taken from the \"Introduction to Neural Networks\" which is part of [Brilliant's](https://brilliant.org/home/) \"CS & Programming\" course. In the course, a cat called Chester is introduced, and we have to predict how Chester will respond when his human gives him attention. When Chester is happy, he purrs. Will Chester purr?\n",
    "\n",
    "![Neuron](perceptron_chester.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7261d655",
   "metadata": {},
   "source": [
    "This is what a perceptron looks like in code. It's surprisingly a small amount of code don't you agree? We will define it here and use it later. It returns $\\hat{y}$ given the input values and coresponding weights. It doesn't care what the numbers mean. It doesn't care that the numbers may corespond to words (or tokens) in the case of a large language model such as [GPT-3](https://en.wikipedia.org/wiki/GPT-3). It doesn't care if the numbers refer to pixels in an image. It doesn't care if we are scratching Chester's belly or Chin. It's just numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f2f43be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(input_values, weights):\n",
    "    output = np.dot(input_values,weights)\n",
    "    output = sigmoid(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30afa3a9",
   "metadata": {},
   "source": [
    "Since we are in code, let's break it down and experiment to further solidify our understanding. The code [np.dot](https://numpy.org/doc/stable/reference/generated/numpy.dot.html) is a shorthand way of calculating the dot-product of our input values and the weights. Let's play around with it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b2335ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "# Assume we have two input values 1 and 10 and coresponding weights of 2 and 3\n",
    "# If we were to do this in long-form, the dot-product would be:\n",
    "dot_product = (1*2)+(10*3)\n",
    "print(dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "621692d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.dot from the NumPy library just does that for us\n",
    "input_values = np.array([1,10])\n",
    "weights = np.array([2,3])\n",
    "np.dot(input_values,weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c01bb3",
   "metadata": {},
   "source": [
    "We then feed that value into our activation function to provide non-linearity - in our case, we are just using the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4fc23c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999873\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid(32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b6375e",
   "metadata": {},
   "source": [
    "Now, let's get back to solving our problem where we are trying to predict if Chester will purr or not. We need someway to encode our knowledge into numbers. To do this, I have a scratch() function which generates an input array of numbers based on if he is being scratched or not. We simply use 0 to denote that Chester is not being scratched and 1 to denote that he is being scratched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "546c7e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scratch(back=False, ears=False, chin=False, belly=False):\n",
    "    input_array  = np.zeros(4)\n",
    "    input_array[0] = 1 if back is True else 0\n",
    "    input_array[1] = 1 if ears is True else 0\n",
    "    input_array[2] = 1 if chin is True else 0\n",
    "    input_array[3] = 1 if belly is True else 0\n",
    "    return input_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab01630",
   "metadata": {},
   "source": [
    "We also need to define the weights. Again, we hard-code these here based on our knowledge of Chester's behavior for example only. Realistically, we would not be setting these weights manually - this is where we rely on machine learning techniques to set the weights during the training of the model. We basically assign higher numbers to represent the things Chester loves and lower (negative) numbers for the things Chester hates making him less likely to purr. The numbers don't really matter - it only matters how the inputs relate to each other. I could for example have chosen much bigger numbers for the things he hates as long as the thing he loves the most is the biggest and hence heavily weighted number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b4de0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume our neuron has been trained based on \n",
    "# the following observations:\n",
    "#\n",
    "# - Chester loves his back being scratched\n",
    "# - Chester hates his ears being scratched\n",
    "# - Chester really likes his chin being scratched\n",
    "# - Chester likes his belly being scratched\n",
    "#\n",
    "# The array is in this order \n",
    "# [back, ears, chin, belly]\n",
    "\n",
    "weights = np.array([3,-10,2,1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85c8383",
   "metadata": {},
   "source": [
    "Now, everything is in place to run inference. Let's do some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35c14836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will Chester the cat purr if I do nothing? 50.00% probability\n"
     ]
    }
   ],
   "source": [
    "input_layer = scratch(back=False, ears=False, chin=False, belly=False)\n",
    "output = perceptron(input_layer, weights)\n",
    "print(\"Will Chester the cat purr if I do nothing? {:.2%} probability\".format(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9e2a88",
   "metadata": {},
   "source": [
    "Recall that Chester **loves his back being scratched**. What does our AI predict when we do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52adf4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will Chester the cat purr if I scratch just his back? 95.26% probability\n"
     ]
    }
   ],
   "source": [
    "input_layer = scratch(back=True, ears=False, chin=False, belly=False)\n",
    "output = perceptron(input_layer, weights)\n",
    "print(\"Will Chester the cat purr if I scratch just his back? {:.2%} probability\".format(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f719190",
   "metadata": {},
   "source": [
    "Recall that Chester **hates his ears being scratched**. What does our AI predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af259710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will Chester the cat purr if I scratch just his ears? 0.00% probability\n"
     ]
    }
   ],
   "source": [
    "input_layer = scratch(back=False, ears=True, chin=False, belly=False)\n",
    "output = perceptron(input_layer, weights)\n",
    "print(\"Will Chester the cat purr if I scratch just his ears? {:.2%} probability\".format(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14ca63b",
   "metadata": {},
   "source": [
    "Recall that Chester **really likes his chin being scratched**. What does our AI predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0aa83be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will Chester the cat purr if I scratch just his chin? 88.08% probability\n"
     ]
    }
   ],
   "source": [
    "input_layer = scratch(back=False, ears=False, chin=True, belly=False)\n",
    "output = perceptron(input_layer, weights)\n",
    "print(\"Will Chester the cat purr if I scratch just his chin? {:.2%} probability\".format(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1d3c06",
   "metadata": {},
   "source": [
    "Recall that Chester **likes his belly being scratched**. What does our AI predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bd2a1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will Chester the cat purr if I scratch just his belly? 73.11% probability\n"
     ]
    }
   ],
   "source": [
    "input_layer = scratch(back=False, ears=False, chin=False, belly=True)\n",
    "output = perceptron(input_layer, weights)\n",
    "print(\"Will Chester the cat purr if I scratch just his belly? {:.2%} probability\".format(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e50e13a",
   "metadata": {},
   "source": [
    "Let's now go for broke. Let's avoid all the things Chester hates and scratch all the things he likes, really likes or loves. What does our AI predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb483a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will Chester the cat purr if I scratch everything except the ears? 99.75% probability\n"
     ]
    }
   ],
   "source": [
    "input_layer = scratch(back=True, ears=False, chin=True, belly=True)\n",
    "output = perceptron(input_layer, weights)\n",
    "print(\"Will Chester the cat purr if I scratch everything except the ears? {:.2%} probability\".format(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60be633d",
   "metadata": {},
   "source": [
    "And that is all there is to the magic of AI (articial neural networks to be exact) at the cellular level. I hope you have found it useful. Now, to go from here you have so many options. There are tons of YouTube videos, books, on-line courses to choose from. Happy learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
